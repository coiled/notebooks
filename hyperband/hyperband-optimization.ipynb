{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization with Dask and Coiled\n",
    "\n",
    "This example will walk through the following:\n",
    "\n",
    "* **Getting and processing the data.**\n",
    "* **Defining a model and parameters.**\n",
    "* **Finding the best parameters,** and some details on why we're using the chosen search algorithm.\n",
    "* **Scoring** and deploying.\n",
    "\n",
    "All of these tasks will be performed on the New York City Taxi Cab dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cluster with Coiled\n",
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    n_workers=20,\n",
    "    software=\"examples/hyperband-optimization\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect Dask to the cluster\n",
    "import dask.distributed\n",
    "\n",
    "client = dask.distributed.Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ☝️ Don’t forget to click the \"Dashboard\" link above to view the cluster dashboard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will mirror the Kaggle \"[NYC Taxi Trip Duration][1]\" example with different data.\n",
    "\n",
    "These data have records on 84 million taxi rides.\n",
    "\n",
    "[1]:https://www.kaggle.com/c/nyc-taxi-trip-duration/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "features = [\"passenger_count\", \"trip_distance\", \"fare_amount\"]\n",
    "categorical_features = [\"RatecodeID\", \"payment_type\"]\n",
    "output = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]\n",
    "\n",
    "df = dd.read_csv(\n",
    "    \"s3://nyc-tlc/trip data/yellow_tripdata_2019-*.csv\", \n",
    "    parse_dates=output,\n",
    "    usecols=features + categorical_features + output,\n",
    "    dtype={\n",
    "        \"passenger_count\": \"UInt8\",\n",
    "        \"RatecodeID\": \"category\",\n",
    "        \"payment_type\": \"category\",\n",
    "    },\n",
    "    blocksize=\"16 MiB\",\n",
    ")\n",
    "\n",
    "df = df.repartition(partition_size=\"10 MiB\").persist()\n",
    "\n",
    "# one hot encode the categorical columns\n",
    "df = df.categorize(categorical_features)\n",
    "df = dd.get_dummies(df, columns=categorical_features)\n",
    "\n",
    "# persist so only download once\n",
    "df = df.persist()\n",
    "\n",
    "data = df[[c for c in df.columns if c not in output]]\n",
    "data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = (df[\"tpep_dropoff_datetime\"] - df[\"tpep_pickup_datetime\"]).dt.total_seconds() / 60  # minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "import dask\n",
    "\n",
    "X = data.to_dask_array(lengths=True).astype(\"float32\")\n",
    "y = durations.to_dask_array(lengths=True).astype(\"float32\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2, shuffle=True)\n",
    "\n",
    "# persist the data so it's not re-computed\n",
    "X_train, X_test, y_train, y_test = dask.persist(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a simple neural network from [PyTorch] using [Skorch], a simple wrapper that provides a Scikit-Learn API for PyTorch.\n",
    "\n",
    "This network is only small for demonstration. If desired, we could use much larger networks on GPUs.\n",
    "\n",
    "[PyTorch]:https://pytorch.org/\n",
    "[skorch]:https://skorch.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our HiddenLayerNet pytorch model from a local torch_model.py module\n",
    "from torch_model import HiddenLayerNet\n",
    "# Send module with HiddenLayerNet to workers on cluster\n",
    "client.upload_file(\"torch_model.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print contents of torch_model.py module\n",
    "!cat torch_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from skorch import NeuralNetRegressor\n",
    "\n",
    "niceties = {\n",
    "    \"callbacks\": False,\n",
    "    \"warm_start\": True,\n",
    "    \"train_split\": None,\n",
    "    \"max_epochs\": 1,\n",
    "}\n",
    "\n",
    "class NonNanLossRegressor(NeuralNetRegressor):\n",
    "    def get_loss(self, y_pred, y_true, X=None, training=False):\n",
    "        if torch.abs(y_true - y_pred).abs().mean() > 1e6:\n",
    "            return torch.tensor([0.0], requires_grad=True)\n",
    "        return super().get_loss(y_pred, y_true, X=X, training=training)\n",
    "\n",
    "model = NonNanLossRegressor(\n",
    "    module=HiddenLayerNet,\n",
    "    module__n_features=X_train.shape[1],\n",
    "    optimizer=optim.SGD,\n",
    "    criterion=nn.MSELoss,\n",
    "    lr=0.0001,\n",
    "    **niceties,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform, uniform\n",
    "\n",
    "params = {\n",
    "    \"module__activation\": [\"relu\", \"elu\", \"softsign\", \"leaky_relu\", \"rrelu\"],\n",
    "    \"batch_size\": [32, 64, 128, 256],\n",
    "    \"optimizer__lr\": loguniform(1e-4, 1e-3),\n",
    "    \"optimizer__weight_decay\": loguniform(1e-6, 1e-3),\n",
    "    \"optimizer__momentum\": uniform(0, 1),\n",
    "    \"optimizer__nesterov\": [True],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these parameters control model architecture, execpt for two basic optimizatino parameters, `batch_size` and `learning_rate_init`. They control finding the best model of a particular architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our search is \"computationally-constrained\" because (hypothetically) it requires GPUs and has a pretty complicated search space (in reality it has neither of those features). And obviously it's \"memory-constrained\" because the dataset doesn't fit in memory.\n",
    "\n",
    "[Dask-ML's documentation on hyperparameter searches][2] indicates that we should use `HyperbandSearchCV`.\n",
    "\n",
    "[2]:https://ml.dask.org/hyper-parameter-search.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import HyperbandSearchCV\n",
    "search = HyperbandSearchCV(model, params, random_state=2, verbose=True, max_iter=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `HyperbandSearchCV` will call `partial_fit` on each chunk of the Dask Array. `HyperbandSearchCV`'s rule of thumb specifies how to train for longer or sample more parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train2 = y_train.reshape(-1, 1).persist()\n",
    "search.fit(X_train, y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HyperbandSearchCV` and the like mirror the Scikit-Learn model selection interface, so all attributes of Scikit-Learn's [RandomizedSearchCV][rscv] are available:\n",
    "\n",
    "[rscv]:https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we can deploy the best model and score on the testing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.wrappers import ParallelPostFit\n",
    "deployed_model = ParallelPostFit(search.best_estimator_)\n",
    "deployed_model.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
